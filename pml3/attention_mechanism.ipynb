{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35423a9",
   "metadata": {},
   "source": [
    "# The Attention Mechanism\n",
    "We have seen how a RNN layer implements a sort of memory by the recursive use of its output with the input. This memory is nonetheless limited in particular for dataset for which an element may depend on other elements and not only on the previous one. An attention mechanism allows a model to access all the elements in a sequence and to weight their relevance for the prediction of the next one. In NLP an attention mechanism should weight more the relevant elements of a sentence, such as subject, verb and object, wherever they are in the sentence, that is the model has learnt the grammar from the examples used to train it.\n",
    "![The attention mechanism](images/attention_mechanism.jpg)\n",
    "\n",
    "A graphical representation of the attention mechanism as proposed in the paper \"[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\" by D. Bahdanau, K.H. Cho and Y. Bengio is shown in the picture. My interpretation of this very relevant paper is the following.\n",
    "\n",
    "A neural network learns the conditional probability distribution that links an input to the output. What has to be learnt by the network during its training are the parameters of the distribution, aka model. The model should represent \n",
    "\n",
    "1. The dependency of each element from the previous and the following elements in the input and output sequences\n",
    "2. The weights between the elements in the input sequence and those in the output sequence\n",
    "\n",
    "The first requirement can be implemented as a RNN or LSTM layer. The second requirement can be implemented as a matrix. The model's parameters are computed from the training examples using the backpropagation algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f4321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
