{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35423a9",
   "metadata": {},
   "source": [
    "# The Attention Mechanism\n",
    "We have seen how a RNN layer implements a sort of memory by the recursive use of its output with the input. This memory is nonetheless limited in particular for dataset for which an element may depend on other elements and not only on the previous one. An attention mechanism allows a model to access all the elements in a sequence and to weight their relevance for the prediction of the next one. In NLP an attention mechanism should weight more the relevant elements of a sentence, such as subject, verb and object, wherever they are in the sentence, that is the model has learnt the grammar from the examples used to train it.\n",
    "![The attention mechanism](images/attention_mechanism.jpg)\n",
    "\n",
    "A graphical representation of the attention mechanism as proposed in the paper \"[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\" by D. Bahdanau, K.H. Cho and Y. Bengio is shown in the picture. My interpretation of this very relevant paper is the following.\n",
    "\n",
    "A neural network learns the conditional probability distribution that links an input to the output. What has to be learnt by the network during its training are the parameters of the distribution, aka model. The model should represent \n",
    "\n",
    "1. The dependency of each element from the previous and the following elements in the input and output sequences\n",
    "2. The weights between the elements in the input sequence and those in the output sequence\n",
    "\n",
    "The first requirement can be implemented as a RNN or LSTM layer. The second requirement can be implemented as a matrix and basically defines the attention mechanism. The model's parameters are computed from the training examples using the backpropagation algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71b6a5",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "In this section we build a small example of an attention mechanism that will provide us a basic intuition to better understand the mechanism itself and the transformer architecture. It has been found that the attention mechanism works better without the recurrent component. The input sequence is processed all at once and the modified mechanism is called self-attention. The architecture with this modification is known as transformer. The step to implement the self-attention mechanism is divided into three steps\n",
    "\n",
    "1. Compute the semantic similarity $\\omega_{i}$ between the embedding $x^{(i)}$ of an element and all the other elements of the sequence\n",
    "2. Compute the attention weights $\\alpha_{ij}$ by computing the softmax of the similarity values\n",
    "3. Use the attention weights and the elements of the sequence to compute the context-aware vectors $z^{(i)}$\n",
    "\n",
    "$$\\omega_{i,j} = x^{(i)} \\cdot x^{(j)}$$\n",
    "\n",
    "$$\\alpha_{i,j} = \\frac{exp(\\omega_{i,j})}{\\sum_{j=1}^T exp(\\omega_{i,j})}$$\n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^T \\alpha_{i,j} x^{(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125ef28",
   "metadata": {},
   "source": [
    "Let's assume we have indexed of a set of words that constitute a sentence, that is a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7112be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 7, 1, 2, 5, 6, 4, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# input sequence / sentence:\n",
    "#  \"Can you help me to translate this sentence\"\n",
    "\n",
    "sentence = torch.tensor(\n",
    "    [0, # can\n",
    "     7, # you     \n",
    "     1, # help\n",
    "     2, # me\n",
    "     5, # to\n",
    "     6, # translate\n",
    "     4, # this\n",
    "     3] # sentence\n",
    ")\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cfc37",
   "metadata": {},
   "source": [
    "We compute the embeddings of these words, a representation of the words in a d=16 dimensional space $\\mathbb{R}^{d}$, that is 8 vectors of 16 real numbers. The embedding of a word is a set of parameters, 16 in our case, that provide a meaning of that word simply because similar words are close in this space. PyTorch provide a [function](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) to compute the embeddings. The embeddings are initialized randomly and should be updated during a training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce23252d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d = 16 # dimension of the embeddings space\n",
    "embed = torch.nn.Embedding(10, d)\n",
    "embedded_sentence = embed(sentence).detach()\n",
    "embedded_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e6def",
   "metadata": {},
   "source": [
    "We can compute the semantic similarity between each pair of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c31c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = torch.empty(8, 8)\n",
    "\n",
    "for i, x_i in enumerate(embedded_sentence):\n",
    "    for j, x_j in enumerate(embedded_sentence):\n",
    "        omega[i, j] = torch.dot(x_i, x_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bc508",
   "metadata": {},
   "source": [
    "We can achieve the same result but more efficiently by using the matrix multiplication operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f5a41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_mat = embedded_sentence.matmul(embedded_sentence.T)\n",
    "omega_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74abe8f",
   "metadata": {},
   "source": [
    "We can see that the result is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c3ad19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(omega_mat, omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b29e2",
   "metadata": {},
   "source": [
    "We compute the attention weights for each word as a function of its semantic similarity with all the other words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b937a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights = F.softmax(omega, dim=1)\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a5da4",
   "metadata": {},
   "source": [
    "With the attention weights we can compute the context-aware vectors of the input word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88addf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = torch.matmul(attention_weights, embedded_sentence)\n",
    "context_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28f889",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "Now we build a slightly more complex example that is closer to the implementation of the attention mechanism in the transformer. In this example we use three matrices to compute three transformation of the embeddings: the query embeddings $q^{(i)}$, the key embeddings $k^{(i)}$, and the value embeddings $v^{(i)}$.\n",
    "\n",
    "$$q^{(i)} = \\hat U_q x^{(i)}$$\n",
    "$$k^{(i)} = \\hat U_k x^{(i)}$$\n",
    "$$v^{(i)} = \\hat U_v x^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03bd01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "U_query = torch.rand(d, d)\n",
    "U_key = torch.rand(d, d)\n",
    "U_value = torch.rand(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8795e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = U_query.matmul(embedded_sentence.T).T\n",
    "keys = U_key.matmul(embedded_sentence.T).T\n",
    "values = U_value.matmul(embedded_sentence.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe68f5",
   "metadata": {},
   "source": [
    "We compute the semantic similarities between the queries and the keys\n",
    "$$\\omega_{ij} = q^{(i)} \\cdot k^{(j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79eebbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega = queries.matmul(keys.T)\n",
    "omega.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ffb2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-25.1623,   9.3602,  14.3667,  32.1482,  53.8976,  46.6626,  -1.2131,\n",
       "        -32.9392])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 = omega[1]\n",
    "omega_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f957a",
   "metadata": {},
   "source": [
    "We compute the normalized attention weights $\\alpha_{ij}$ using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af13473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2317e-09, 1.2499e-05, 4.3696e-05, 3.7242e-03, 8.5596e-01, 1.4026e-01,\n",
       "        8.8897e-07, 3.1935e-10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d**0.5, dim=0)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3875e",
   "metadata": {},
   "source": [
    "Finally we compute the context weighted values \n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^T \\alpha_{ij}v_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91fcd8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2226, -3.4387, -4.3928, -5.2125, -1.1249, -3.3041, -1.4316, -3.2765,\n",
       "        -2.5114, -2.6105, -1.5793, -2.8433, -2.4142, -0.3998, -1.9917, -3.3499])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc267c",
   "metadata": {},
   "source": [
    "## Multi-head attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5f6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
