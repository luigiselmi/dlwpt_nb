{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25010ebb",
   "metadata": {},
   "source": [
    "# Character-level language model\n",
    "<a href=\"https://github.com/luigiselmi/machine_learning_notes/blob/main/pml3/char_level_language_model.ipynb\" target=\"_blank\" align=\"right\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "We want to build a character-level language model based of a RNN that can generate new text with the same style of a text that has been used for training. We download the book \"The Mysterious Island\" by Julius Verne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4006850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-07-14 12:18:19--  https://www.gutenberg.org/files/1268/1268-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1171290 (1.1M) [text/plain]\n",
      "Saving to: 'data/1268-0.txt.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4%  134K 8s\n",
      "    50K .......... .......... .......... .......... ..........  8%  242K 6s\n",
      "   100K .......... .......... .......... .......... .......... 13% 11.5M 4s\n",
      "   150K .......... .......... .......... .......... .......... 17% 13.7M 3s\n",
      "   200K .......... .......... .......... .......... .......... 21%  244K 3s\n",
      "   250K .......... .......... .......... .......... .......... 26% 23.1M 2s\n",
      "   300K .......... .......... .......... .......... .......... 30%  259K 2s\n",
      "   350K .......... .......... .......... .......... .......... 34% 12.4M 2s\n",
      "   400K .......... .......... .......... .......... .......... 39% 6.25M 2s\n",
      "   450K .......... .......... .......... .......... .......... 43%  254K 2s\n",
      "   500K .......... .......... .......... .......... .......... 48% 6.48M 1s\n",
      "   550K .......... .......... .......... .......... .......... 52% 18.1M 1s\n",
      "   600K .......... .......... .......... .......... .......... 56% 15.6M 1s\n",
      "   650K .......... .......... .......... .......... .......... 61% 10.7M 1s\n",
      "   700K .......... .......... .......... .......... .......... 65%  264K 1s\n",
      "   750K .......... .......... .......... .......... .......... 69% 14.0M 1s\n",
      "   800K .......... .......... .......... .......... .......... 74% 20.0M 0s\n",
      "   850K .......... .......... .......... .......... .......... 78%  245K 0s\n",
      "   900K .......... .......... .......... .......... .......... 83% 8.00M 0s\n",
      "   950K .......... .......... .......... .......... .......... 87% 9.25M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 91% 10.5M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 96% 9.81M 0s\n",
      "  1100K .......... .......... .......... .......... ...       100% 24.8M=1.6s\n",
      "\n",
      "2023-07-14 12:18:22 (698 KB/s) - 'data/1268-0.txt.1' saved [1171290/1171290]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://www.gutenberg.org/files/1268/1268-0.txt' -P data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80689862",
   "metadata": {},
   "source": [
    "We use the text from the title and count the total number of characters and the number of unique characters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4695679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1130711\n",
      "Unique Characters: 85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Reading and processing text\n",
    "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b825e2e",
   "metadata": {},
   "source": [
    "We encode the character in the text as integers. The integer values can be decoded into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e1f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17df08e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 -> T\n",
      "36 -> H\n",
      "33 -> E\n",
      "1 ->  \n",
      "41 -> M\n",
      "53 -> Y\n",
      "47 -> S\n",
      "48 -> T\n",
      "33 -> E\n",
      "46 -> R\n",
      "37 -> I\n",
      "43 -> O\n",
      "49 -> U\n",
      "47 -> S\n",
      "1 ->  \n",
      "37 -> I\n",
      "47 -> S\n",
      "40 -> L\n",
      "29 -> A\n",
      "42 -> N\n",
      "32 -> D\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:21]:\n",
    "    print('{} -> {}'.format(ex, char_array[ex]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de59b4",
   "metadata": {},
   "source": [
    "The text generation task, where a sequence of characters are used to infer the next one, can be thought as a multiclass classification task where an incomplete text is mapped (i.e. classified) to one of the character in our alphabet of unique characters. We create the training set using sequences of characters from the text and as label the character immediately after the last character. We choose the lenght of the sequences to be 41, 40 for the sequences and 1 for the label, that is the character after each sequence. Our model should allows as to create new sequences and the labels.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be3c2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48 36 33  1 41 53 47 48 33 46 37 43 49 47  1 37 47 40 29 42 32  1 10 10\n",
      " 10  0  0  0  0  0 48 36 33  1 41 53 47 48 33 46]  ->  37\n",
      "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'  ->  'I'\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "text_chunks = [text_encoded[i:i+chunk_size] \n",
    "               for i in range(len(text_encoded)-chunk_size+1)] \n",
    "\n",
    "## inspection:\n",
    "for seq in text_chunks[:1]:\n",
    "    input_seq = seq[:seq_length]\n",
    "    target = seq[seq_length] \n",
    "    print(input_seq, ' -> ', target)\n",
    "    print(repr(''.join(char_array[input_seq])), \n",
    "          ' -> ', repr(''.join(char_array[target])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308f200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luigi\\AppData\\Local\\Temp\\ipykernel_20900\\2527503007.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32057bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      " Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
    "    print('Target (y):', repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d280f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45989154",
   "metadata": {},
   "source": [
    "## Model definition and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69c468cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(85, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell\n",
    "    \n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d45964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.4364\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 10000 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    #seq_batch = seq_batch.to(device)\n",
    "    #target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell) \n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()/seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08407689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
