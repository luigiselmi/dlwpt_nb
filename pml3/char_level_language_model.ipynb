{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3af65cb",
      "metadata": {
        "id": "a3af65cb"
      },
      "source": [
        "# Character-level language model\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/luigiselmi/machine_learning_notes/blob/main/pml3/char_level_language_model.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "We want to build a character-level language model based of a RNN that can generate new text with the same style of a text that has been used for training. We download the book \"The Mysterious Island\" by Julius Verne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "85c3c52e",
      "metadata": {
        "scrolled": true,
        "id": "85c3c52e",
        "outputId": "649ac3e3-11aa-4bd5-80d1-b13ee77f39a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-27 12:54:27--  https://www.gutenberg.org/files/1268/1268-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1171290 (1.1M) [text/plain]\n",
            "Saving to: ‘data/1268-0.txt’\n",
            "\n",
            "1268-0.txt          100%[===================>]   1.12M   879KB/s    in 1.3s    \n",
            "\n",
            "2023-07-27 12:54:30 (879 KB/s) - ‘data/1268-0.txt’ saved [1171290/1171290]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://www.gutenberg.org/files/1268/1268-0.txt' -P data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd83692f",
      "metadata": {
        "id": "cd83692f"
      },
      "source": [
        "We use the text from the title and count the total number of characters and the number of unique characters used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2bdb8c73",
      "metadata": {
        "id": "2bdb8c73",
        "outputId": "42a2f592-53cd-449c-eaaa-bfbbcccb7e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Length: 1130711\n",
            "Unique Characters: 85\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "## Reading and processing text\n",
        "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('Total Length:', len(text))\n",
        "print('Unique Characters:', len(char_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a74d29e0",
      "metadata": {
        "id": "a74d29e0"
      },
      "source": [
        "We encode the characters in the text as integers. The integer values can be decoded into characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e19e2c1",
      "metadata": {
        "id": "6e19e2c1",
        "outputId": "8b5b2e04-8f61-4ca5-89a3-0f1f1ffc05d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char2int['A']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "937ef1ed",
      "metadata": {
        "id": "937ef1ed",
        "outputId": "0540ea3e-d1de-4e23-c2ef-36eb6e4b46ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
              "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
              "      dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "char_array = np.array(chars_sorted)\n",
        "char_array[29:81]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "87bd789a",
      "metadata": {
        "id": "87bd789a",
        "outputId": "439069bf-9060-4a9f-f00f-c14d36d47abc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (1130711,)\n",
            "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
            "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a7b14a82",
      "metadata": {
        "id": "a7b14a82",
        "outputId": "3d7d4ff7-ea04-4d2e-9378-2900d6c85a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 -> T\n",
            "36 -> H\n",
            "33 -> E\n",
            "1 ->  \n",
            "41 -> M\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7efaa4",
      "metadata": {
        "id": "da7efaa4"
      },
      "source": [
        "## Character prediction as a multiclass classification task\n",
        "The text generation task, where a sequence of characters are used to infer the next one, can be thought as a multiclass classification task where an incomplete text is mapped (i.e. classified) to one of the characters in our alphabet of unique characters. We create the training set using sequences of characters from the text and as label the character immediately after the last character. We choose the lenght of the sequences to be 41, 40 for the sequences and 1 for the label, that is the character after each sequence. Our model should allows us to create new sequences with the labels.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5af12e",
      "metadata": {
        "id": "ba5af12e"
      },
      "source": [
        "We create chunks of characters of length 40 from the text with the following character after the chunk used as label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4f63a3e0",
      "metadata": {
        "id": "4f63a3e0"
      },
      "outputs": [],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i + chunk_size] for i in range(len(text_encoded) - chunk_size + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a6c94cc",
      "metadata": {
        "id": "2a6c94cc",
        "outputId": "b8a25802-6c84-4bf8-fb05-c74309b9a4e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48, 36, 33,  1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1, 37, 47,\n",
              "       40, 29, 42, 32,  1, 10, 10, 10,  0,  0,  0,  0,  0, 48, 36, 33,  1,\n",
              "       41, 53, 47, 48, 33, 46, 37], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b13dd145",
      "metadata": {
        "id": "b13dd145",
        "outputId": "96affb9f-6b17-4387-9861-151b8098b181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48 36 33  1 41 53 47 48 33 46 37 43 49 47  1 37 47 40 29 42 32  1 10 10\n",
            " 10  0  0  0  0  0 48 36 33  1 41 53 47 48 33 46]  ->  37\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'  ->  'I'\n"
          ]
        }
      ],
      "source": [
        "## inspection:\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:seq_length]\n",
        "    target = seq[seq_length]\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])), ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29150556",
      "metadata": {
        "id": "29150556"
      },
      "source": [
        "We create a PyTorch dataset from the set of chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c765b20a",
      "metadata": {
        "id": "c765b20a",
        "outputId": "0186db1d-b6d1-4ac8-9a50-254d959f4769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-178b9785e257>:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7e724f69",
      "metadata": {
        "id": "7e724f69",
        "outputId": "d429986f-ee2c-4748-e61c-4a227b99342e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
            "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "\n",
            " Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
            "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
        "    print('Target (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2aed5d",
      "metadata": {
        "id": "fb2aed5d"
      },
      "source": [
        "We create batches from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "772be4f9",
      "metadata": {
        "id": "772be4f9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027b7e37",
      "metadata": {
        "id": "027b7e37"
      },
      "source": [
        "## Model definition and training\n",
        "We define a model that contains one LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c52ba7eb",
      "metadata": {
        "id": "c52ba7eb",
        "outputId": "87930749-f484-466c-a9e7-990b89e111d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(85, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb15923",
      "metadata": {
        "id": "1cb15923"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "08864fc4",
      "metadata": {
        "id": "08864fc4",
        "outputId": "29a8557c-e7fe-44fa-c28c-0ca568cf3be2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 4.4364\n",
            "Epoch 500 loss: 1.3689\n",
            "Epoch 1000 loss: 1.2488\n",
            "Epoch 1500 loss: 1.2320\n",
            "Epoch 2000 loss: 1.2105\n",
            "Epoch 2500 loss: 1.1728\n",
            "Epoch 3000 loss: 1.2025\n",
            "Epoch 3500 loss: 1.1637\n",
            "Epoch 4000 loss: 1.1664\n",
            "Epoch 4500 loss: 1.1293\n",
            "Epoch 5000 loss: 1.1227\n",
            "Epoch 5500 loss: 1.1523\n",
            "Epoch 6000 loss: 1.1511\n",
            "Epoch 6500 loss: 1.0963\n",
            "Epoch 7000 loss: 1.1509\n",
            "Epoch 7500 loss: 1.1887\n",
            "Epoch 8000 loss: 1.1368\n",
            "Epoch 8500 loss: 1.0813\n",
            "Epoch 9000 loss: 1.1323\n",
            "Epoch 9500 loss: 1.1442\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    #seq_batch = seq_batch.to(device)\n",
        "    #target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59596b7",
      "metadata": {
        "id": "e59596b7"
      },
      "source": [
        "## Evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d03b6639",
      "metadata": {
        "id": "d03b6639",
        "outputId": "420ff27d-a508-4af5-8c20-88144b49c681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
        "\n",
        "print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4cda4e42",
      "metadata": {
        "id": "4cda4e42",
        "outputId": "e7f2c2d7-ab7a-47cb-93b3-fbe6d188a173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
            "[[0]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str,\n",
        "           len_generated_text=500,\n",
        "           scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ],
      "metadata": {
        "id": "OZe5e-aO4n4P",
        "outputId": "569f5f24-4a9d-4145-8b58-a09f28144dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OZe5e-aO4n4P",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island is\n",
            "the\n",
            "current over the sig or needlessures at a wands on the river, and you bow, sincident, they had-fact, since day of destronic avoct a fibird, and without acride as well alone camped by this occurinuishy violent. December, Pencroft, he must Neb,” said Pencroft, as Pencroft became necessary to any effect of a disagreen thus were\n",
            "beautiful art, a castaway had been state of a kinds.\n",
            "\n",
            "The summit methodical of the other what course, mingled graratus. Every\n",
            "anxiety, at Top was left the hasty loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictability vs Randomness"
      ],
      "metadata": {
        "id": "pRIihUfO4-oS"
      },
      "id": "pRIihUfO4-oS"
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('Probabilities before scaling:        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.5:', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
        "\n",
        "print('Probabilities after scaling with 0.1:', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
      ],
      "metadata": {
        "id": "l-mO39mg40xl",
        "outputId": "f03f2a21-1e0e-48ef-9bb2-72db9ff2994f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l-mO39mg40xl",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities before scaling:         [0.10650698 0.10650698 0.78698605]\n",
            "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611686]\n",
            "Probabilities after scaling with 0.1: [0.3104238  0.3104238  0.37915248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island', scale_factor=2.0))"
      ],
      "metadata": {
        "id": "dk4bQBlO480d",
        "outputId": "00d15a43-770c-45e4-8864-0710f925fcc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dk4bQBlO480d",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island is of a few of the colonists became a lumage was there and his describence was there that he will be discovered to find the sea.\n",
            "\n",
            "The colonists are not attached in the water of the air will be seen.\n",
            "\n",
            "“We shall be surveyed, and it was now seen in the middle of the windows of the colonists buried at Tabor Island and showing the summit of the sea. The colonists had carried at the same last arrively made the engineer had been satiged the capybara regularly examine that the birds were to be seen fro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island', scale_factor=0.5))"
      ],
      "metadata": {
        "id": "-48hj9pG5QSr",
        "outputId": "9287671d-4742-4179-d576-772529f870df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-48hj9pG5QSr",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island?\n",
            "Eughousu wile hear Chip bodmen percerturds. Each\n",
            "goat jawific dam, 367d you...\n",
            "vesireir.\n",
            "\n",
            "But downed-first\n",
            "sistaneful, in it benon fout. Whatever\n",
            "he knewch ficae-well; nextry?\n",
            "Had this, chock,---41. ifuage\n",
            "nestwaid. Dby reckoning. Nempwhered sneep,”.s\n",
            "Lissualjt, I’ll Perhum’ rose. Gonbrejorcy to-molde-, old, tay!\n",
            "Grebrdiute azotic\n",
            "beaus litea” had cartankly duractrHulacely walkings..W--arsquescrmed hoursed!.--buinted\n",
            "druttleshs all, Ayrtaugrs, pudrected\n",
            "yas.” It\n",
            "satcribilizely.\n",
            "It will regu, a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tvFZ_Yi5YBs"
      },
      "id": "_tvFZ_Yi5YBs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}