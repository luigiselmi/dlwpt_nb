{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12cb86d",
   "metadata": {},
   "source": [
    "# PyTorch Computational Graph and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00683595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.23.1\n",
      "Pandas version: 1.4.3\n",
      "PyTorch version: 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"NumPy version: %s\"%np.__version__)\n",
    "print(\"Pandas version: %s\"%pd.__version__)\n",
    "print(\"PyTorch version: %s\"%torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bad7c",
   "metadata": {},
   "source": [
    "We build a graph for a very simple computation: \n",
    "\n",
    "$$s = 2 * (x - y) + z$$\n",
    "\n",
    "The computational graph is a directed acyclic graph that can be built from operators such as sub(), mul(), add() that implement the arithmetic operations on scalars: subtraction, multiplication, addition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc225b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_s(x, y, z):\n",
    "    r1 = torch.sub(x, y)\n",
    "    r2 = torch.mul(r1, 2)\n",
    "    z = torch.add(r2, z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29426b",
   "metadata": {},
   "source": [
    "We assign a value to the variables that are represented as tensors of rank 0 and then we compute the value of the dependent variable s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d29a8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = tensor(3)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1)\n",
    "y = torch.tensor(2)\n",
    "z = torch.tensor(5)\n",
    "s = compute_z(a, b, c)\n",
    "print('s =', s)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0429f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4183,  0.1688,  0.0390],\n",
      "        [ 0.3930, -0.2858, -0.1051]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "w = torch.empty(2, 3)\n",
    "nn.init.xavier_normal_(w)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e485612",
   "metadata": {},
   "source": [
    "## Backpropagation and autodifferentiation\n",
    "A deep learning model is a parametric model of nested functions, e.g. y = f(g(h(x))), that we use to fit an unknown function $\\hat{y}$, like a function that maps a picture of a cat to its label \"cat\". Fitting this type of models is not different conceptually from fitting a linear model such as y = ax + b. In this simple case we have only two parameters a, b, that we want to know using a set of examples. The procedure is to define a loss function, e.g. $\\mathscr{L} =|| y - \\hat{y}||^2$, that we want to minimize by updating the parameters a, b till the loss is as close to zero as possible. The same is valid also for a deep neural network model. We have a model built from nested parametric nonlinear functions for which we know the form (a combination of affine transformations and well known activation functions) but not the right parameters so the procedure is again to write a loss function and to minimize the loss by updating the model's parameters $w$ using a set of examples and the backpropagation algorithm\n",
    "\n",
    "$$w_{i+1} = w_i - \\gamma \\nabla_w{\\mathscr{L}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\nabla_w{\\mathscr{L}} = 2||y - \\hat{y}|| \\frac{\\partial{y}}{\\partial{w}}$$\n",
    "\n",
    "Clearly the y function must be differentiable if we want to compute its gradient, that is its partial derivatives with respect to the parameters $w$. In case of a simple model we could write the equations to compute the derivatives numerically or by symbolic substitution and the chain rule known from Calculus. The problem is that with deep neural networks with thousands or millions of parameters this procedure would be unfeasible and numerically unstable. A better technique is computing the derivatives of the y function with respect to the model parameters using the computational graph that is built with the model. When we define a model PyTorch builds the corresponding computational graph. In order to compute the derivatives with respect to a paramenter we have to tell PyTorch which parameters require the derivative (or the gradient if it is an array). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6399f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
