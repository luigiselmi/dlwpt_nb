{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7467369",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) are used for sequence data, that is data in which each item depends on one or more of the previous items. Examples of this type of data are time series and text. We can look at a simple RNN with one single hidden layer to understand how it works. The input data at time t is sent to the output layer through the hidden layer and also back to the hidden layer to be used in combination with the next input. The loop works like a memory and allows the network to learn the dependency between elements in the sequence. \n",
    "\n",
    "![RNN - Wikipedia, By fdeloche - Own work, CC BY-SA 4.0](images/recurrent_neural_network.svg)\n",
    "(Image from Wikipedia. Credit: By fdeloche - Own work, CC BY-SA 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3572378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.23.1\n",
      "Pandas version: 1.4.3\n",
      "PyTorch version: 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"NumPy version: %s\"%np.__version__)\n",
    "print(\"Pandas version: %s\"%pd.__version__)\n",
    "print(\"PyTorch version: %s\"%torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751ed03",
   "metadata": {},
   "source": [
    "We can compute the preactivation of the hidden layer by two matrix multiplications, one to weight the input data and another to weight the result of the previous input data.\n",
    "\n",
    "$$z_h^{(t)} = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$$\n",
    "\n",
    "The output of the hidden layer is then computed by applying an activation function $\\sigma_h$ to the result of the preactivation\n",
    "\n",
    "$$h^{(t)} = \\sigma_h (W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8bac6",
   "metadata": {},
   "source": [
    "Now we implement a small RNN with one hidden layer. Let's say the input data is an array of size 5 so that the input size of the hidden layer is 5. We set the size of the output of the hidden layer to be an array of size 2. With these settings it's easy to compute the shape of the $W_{xh}$ matrix to be 2x5 and the shape of the $W_{hh}$ matrix to be 2x2. The [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) implementation in PyTorch builds the matrices from the same parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea91bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True) \n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097c7f2",
   "metadata": {},
   "source": [
    "We can compute the output of the RNN instance for a sequence of three inputs and compare the result with that computed using the formula we have described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49dbb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0225706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[-0.4701929  0.5863904]]\n",
      "   Output (manual) : [[-0.3519801   0.52525216]]\n",
      "   RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[-0.88883156  1.2364397 ]]\n",
      "   Output (manual) : [[-0.68424344  0.76074266]]\n",
      "   RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[-1.3074701  1.886489 ]]\n",
      "   Output (manual) : [[-0.8649416   0.90466356]]\n",
      "   RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8043e",
   "metadata": {},
   "source": [
    "Like the other neural networks that we have seen so far, the weights in a RNN are learnt through backpropagation. The loop introduced in a RNN with many layers may result in one of two opposite problems: exploding gradients or vanishing gradients. The two outcomes depend on the value of the $W_{hh}$ matrix that are computed multiple times depending on the lenght of the sequence we consider to be relevant for the output. If the $|W_{hh}| > 1$ we may face the problem of exploding gradients, on the contrary if $|W_{hh}| < 1$ we may face the problem of vanishing gradients. These problems can be addressed by limiting the length of the sequence we want to take into account for the output. Another apporach is to use the Long Short-Term Memory cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96a74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
