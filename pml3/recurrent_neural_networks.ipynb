{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749aafbd",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) are used for sequence data, that is data in which each item depends on one or more of the previous items. Examples of this type of data are time series, text, and DNA sequences. We can look at a simple RNN with one single hidden layer to understand how it works. The input data at time t is sent to the output layer through the hidden layer and also back to the hidden layer to be used in combination with the next input. The loop works like a memory and allows the network to learn the dependency between elements in the sequence. \n",
    "\n",
    "![RNN - Wikipedia, By fdeloche - Own work, CC BY-SA 4.0](images/recurrent_neural_network.svg)\n",
    "In the image a RNN with one hidden layer (Credit: fdeloche - Own work, CC BY-SA 4.0, Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a325bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.0\n",
      "Pandas version: 1.5.3\n",
      "PyTorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"NumPy version: %s\"%np.__version__)\n",
    "print(\"Pandas version: %s\"%pd.__version__)\n",
    "print(\"PyTorch version: %s\"%torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40689c",
   "metadata": {},
   "source": [
    "We can compute the preactivation of the hidden layer by two matrix multiplications, one to weight the input data and another to weight the result of the previous input data.\n",
    "\n",
    "$$z_h^{(t)} = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$$\n",
    "\n",
    "The output of the hidden layer is then computed by applying an activation function $\\sigma_h$ to the result of the preactivation\n",
    "\n",
    "$$h^{(t)} = \\sigma_h (z_h^{(t)}) = \\sigma_h (W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23ebcc",
   "metadata": {},
   "source": [
    "## PyTorch RNN implementation\n",
    "Now we implement a small RNN with one hidden layer. Let's say the input data is an array of size 5 so that the size of the input layer is 5. We set the size of the hidden layer to 2. With these settings the shape of the $W_{xh}$ matrix is 2x5 and the shape of the $W_{hh}$ matrix is 2x2. The [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) implementation in PyTorch builds the matrices from the same parameters. We also assume a bias for each unit of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241b94cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True) \n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e9dc9",
   "metadata": {},
   "source": [
    "We can compute the output of the RNN instance for a sequence of three inputs and compare the result with that computed using the formula we have described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9573c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fae7e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[-0.4701929  0.5863904]]\n",
      "   Output (manual) : [[-0.3519801   0.52525216]]\n",
      "   RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[-0.88883156  1.2364397 ]]\n",
      "   Output (manual) : [[-0.68424344  0.76074266]]\n",
      "   RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[-1.3074701  1.886489 ]]\n",
      "   Output (manual) : [[-0.8649416   0.90466356]]\n",
      "   RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c712e",
   "metadata": {},
   "source": [
    "Like the other neural networks that we have seen so far, the weights in a RNN are learnt through backpropagation. The loop introduced in a RNN with many layers may result in one of two opposite problems: exploding gradients or vanishing gradients. The problem is discussed in a [paper](https://arxiv.org/abs/1211.5063) by Pascanu, Mikolov, Bengio. The two outcomes depend on the value of the $W_{hh}$ matrix that are computed multiple times depending on the lenght of the sequence we consider to be relevant for the output. If the $|W_{hh}| > 1$ we may face the problem of exploding gradients, on the contrary if $|W_{hh}| < 1$ we may face the problem of vanishing gradients. These problems can be addressed by limiting the length of the sequence we want to take into account for the output. Another approach is to use the Long Short-Term Memory cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e96af",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory network\n",
    "The [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) cell is the equivalent of a layer and solves the problem of the exploding or vanishing gradients by keeping the recurrent edge close to 1. The cell state $C_t$ depends on the previous cell state $C_{t-1}$, the previous output of the hidden units $h_{t-1}$, and on the input in the sequence. The symbol $\\oplus$ in the draw represents the element-wise sumation, and the $\\otimes$ symbol represents the element-wise product. The boxes are called gates and are used to carry out matrix-vector multiplications between the input or the recurrent edge units and the weights to coumpute the preactivations. The result of the preactivation is used by the activation function defined in the cell. The three gates $f_t$, forget gate, $i_t$ input gate, and $o_t$ output gate, use a sigmoid activation function ($\\sigma$).\n",
    "\n",
    "![LSTM](images/long_short-term_memory.svg)\n",
    "(Credit: fdeloche - Own work, Wikipedia, CC BY-SA 4.0)\n",
    "\n",
    "$$f_t = \\sigma(W_{xf}x^{(t)} + W_{hf}h^{(t-1)} + b_f)$$\n",
    "$$i_t = \\sigma(W_{xi}x^{(t)} + W_{hi}h^{(t-1)} + b_i)$$\n",
    "$$\\tilde{C_t} = tanh(W_{xc}x^{(t)} + W_{hc}h^{(t-1)} + b_c)$$\n",
    "$$C^{(t)} = (C^{(t-1)} \\otimes f_t) \\oplus (i_t \\otimes \\tilde{C_t})$$\n",
    "$$o_t = \\sigma(W_{xo}x^{(t)} + W_{ho}h^{(t-1)} + b_o)$$\n",
    "$$h^{(t)} = o_t \\otimes tanh(C^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb3dce",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "We use the PyTorch implementation of the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) to develop a tool to determine the sentiment about movies using a set of reviews that have been left by the public. For this problem we will use a sequence of words to infer the sentiment of the authors. We will use the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) dataset. The dataset must be downloaded and each text file with a review and sentiment is copied on a txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b1b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "movie_df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
    "                reviews_df = pd.concat([reviews_df, x], ignore_index=False)\n",
    "                \n",
    "movie_df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f2b220a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910b264",
   "metadata": {},
   "source": [
    "We might want to save the data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3714c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_df.sample(frac=1, random_state=1).reset_index(drop=True) # returns a randomized dataframe\n",
    "movie_df.to_csv(basepath + '/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7824d79",
   "metadata": {},
   "source": [
    "and then open the file for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5059d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PROM NIGHT (2008)&lt;br /&gt;&lt;br /&gt;directed by: Nels...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let me tell you something...this movie exceeds...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private Practice is supposed to be a medical d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  PROM NIGHT (2008)<br /><br />directed by: Nels...          0\n",
       "1  Let me tell you something...this movie exceeds...          0\n",
       "2  Private Practice is supposed to be a medical d...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath = 'data/aclImdb'\n",
    "movie_df = pd.read_csv(basepath + '/movie_data.csv', encoding='utf-8')\n",
    "movie_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f690aa",
   "metadata": {},
   "source": [
    "We create a custom IMDB dataset from the Python Dataframe. A PyTorch dataset is used to train a model and contains a set of examples, in our case with a set of reviews and sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57703af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        review = item['review']\n",
    "        sentiment = item['sentiment']\n",
    "        if self.transform:\n",
    "            review = self.transform(review)\n",
    "        if self.target_transform:\n",
    "            sentiment = self.target_transform(sentiment)\n",
    "        return review, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffa76",
   "metadata": {},
   "source": [
    "We split the IMDB dataset into a training and test dataset of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e47185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 25000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "movie_dataset = ImdbDataset(df=movie_df)\n",
    "#movie_loader = DataLoader(dataset=review_data, batch_size=4,shuffle=True)\n",
    "\n",
    "train_dataset, test_dataset = random_split(list(movie_dataset), [25000, 25000])\n",
    "print('Train dataset: {0:d}\\nTest dataset: {1:d}'.format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5568480",
   "metadata": {},
   "source": [
    "We split the train dataset into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d181fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 20000\n",
      "Validation dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n",
    "print('Train dataset: {0:d}\\nValidation dataset: {1:d}'.format(len(train_dataset), len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2923",
   "metadata": {},
   "source": [
    "We extract the unique words (tokens) from each review in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f63342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5c47530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, sentiment in train_dataset:\n",
    "    tokens = tokenizer(review)\n",
    "    token_counts.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99ab5c",
   "metadata": {},
   "source": [
    "The number of unique words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5ca03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 71086\n"
     ]
    }
   ],
   "source": [
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f8d9",
   "metadata": {},
   "source": [
    "We create a dictionary using the unique words from the reviews as keys and an integer as value. The dictionary (or vocabulary) should be created from the full dataset, not only from the training set, otherwise some words in the valid or test dataset might be missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f673bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "count = 2\n",
    "for item in ordered_dict.items():\n",
    "    key = item[0]\n",
    "    value = item[1]\n",
    "    vocab[key] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16520f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 472]\n"
     ]
    }
   ],
   "source": [
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48985e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(str(x))]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce9acbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 7, 35, 472, 3, 11, 7, 35, 472]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('this is an example and this is an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c6069a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11,   7,  35, 472,   3,  11,   7,  35, 472]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text_list = []\n",
    "processed_text = torch.tensor(text_pipeline('this is an example and this is an example'), dtype=torch.int64)\n",
    "my_text_list.append(processed_text)\n",
    "nn.utils.rnn.pad_sequence(my_text_list, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5176f",
   "metadata": {},
   "source": [
    "We implement a collate_fn function to build a batch from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "019c3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384601a0",
   "metadata": {},
   "source": [
    "We print the first batch of the training set. We can see that the length of each batch is different. Nontheless the number of tokens in each batch is the same since it has been padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "349312d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17949,  7190,     7,  ...,     0,     0,     0],\n",
      "        [   11,     3,  1658,  ...,     0,     0,     0],\n",
      "        [   44,    21,   102,  ...,     0,     0,     0],\n",
      "        [    3,   101,   317,  ...,  2107,    61,   114]])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([182, 231, 228, 491])\n",
      "torch.Size([4, 491])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader)) # takes the first batch\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cab25075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: batching the datasets\n",
    "batch_size = 32  \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab967bc",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "We will represent the words as vectors in a space of real numbers whose dimension n is much smaller than the number of words so that each word can be represented in this space using n real numbers. PyTorch provides some function to create embeddings. For instance we create an embedding of two tensors of integers. Each tensor represents a batch and each integer represents a word. Each word is mapped to a point in the 3-dimensional embedding space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6a3936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4179, -1.1119, -0.8271],\n",
      "         [-0.3051, -0.0225, -0.6329],\n",
      "         [-0.3254, -0.9680,  1.5573],\n",
      "         [-0.1910, -0.9103, -0.9134]],\n",
      "\n",
      "        [[-0.3254, -0.9680,  1.5573],\n",
      "         [ 0.1040,  1.4941,  0.2961],\n",
      "         [-0.3051, -0.0225, -0.6329],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    " \n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a3899",
   "metadata": {},
   "source": [
    "## Building an RNN model\n",
    "We create a model with two RNN layers and a final fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "569e648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1800],\n",
       "        [ 0.0745],\n",
       "        [-0.1253],\n",
       "        [-0.0129],\n",
       "        [-0.1833]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## An example of building a RNN model\n",
    "## with simple RNN layer\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=2, \n",
    "                          batch_first=True)\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(64, 32) \n",
    "\n",
    "print(model) \n",
    " \n",
    "model(torch.randn(5, 3, 64)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e2d42",
   "metadata": {},
   "source": [
    "## Building an RNN model for the sentiment analysis task\n",
    "We use an LSTM layer to take into account long distance dependencies between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3febc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c76e5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6881ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4298992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
