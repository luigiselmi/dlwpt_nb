{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749aafbd",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "<a href=\"https://colab.research.google.com/github/luigiselmi/machine_learning_notes/blob/main/pml3/recurrent_neural_networks.ipynb\" target=\"_blank\" align=\"right\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) are used for sequence data, that is data in which each item depends on one or more of the previous items. Examples of this type of data are time series, text, and DNA sequences. We can look at a simple RNN with one single hidden layer to understand how it works. The input data at time t is sent to the output layer through the hidden layer and also back to the hidden layer to be used in combination with the next input. The loop works like a memory and allows the network to learn the dependency between elements in the sequence. \n",
    "\n",
    "![RNN - Wikipedia, By fdeloche - Own work, CC BY-SA 4.0](images/recurrent_neural_network.svg)\n",
    "In the image a RNN with one hidden layer (Credit: fdeloche - Own work, CC BY-SA 4.0, Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a325bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.0\n",
      "Pandas version: 1.5.3\n",
      "PyTorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"NumPy version: %s\"%np.__version__)\n",
    "print(\"Pandas version: %s\"%pd.__version__)\n",
    "print(\"PyTorch version: %s\"%torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40689c",
   "metadata": {},
   "source": [
    "We can compute the preactivation of the hidden layer by two matrix multiplications, one to weight the input data and another to weight the result of the previous input data.\n",
    "\n",
    "$$z_h^{(t)} = W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h$$\n",
    "\n",
    "The output of the hidden layer is then computed by applying an activation function $\\sigma_h$ to the result of the preactivation\n",
    "\n",
    "$$h^{(t)} = \\sigma_h (z_h^{(t)}) = \\sigma_h (W_{xh}x^{(t)} + W_{hh}h^{(t-1)} + b_h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23ebcc",
   "metadata": {},
   "source": [
    "## PyTorch RNN implementation\n",
    "Now we implement a small RNN with one hidden layer. Let's say the input data is an array of size 5 so that the size of the input layer is 5. We set the size of the hidden layer to 2. With these settings the shape of the $W_{xh}$ matrix is 2x5 and the shape of the $W_{hh}$ matrix is 2x2. The [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) implementation in PyTorch builds the matrices from the same parameters. We also assume a bias for each unit of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241b94cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True) \n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e9dc9",
   "metadata": {},
   "source": [
    "We can compute the output of the RNN instance for a sequence of three inputs (arrays of length 5), and compare the result with that computed using the formula we have described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9573c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "x_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe00bf",
   "metadata": {},
   "source": [
    "We reshape the sequence as a tensor of dimensions: batch_size=1, sequence_length=3, input_size=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911d8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fae7e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[-0.4701929  0.5863904]]\n",
      "   Output (manual) : [[-0.3519801   0.52525216]]\n",
      "   RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[-0.88883156  1.2364397 ]]\n",
      "   Output (manual) : [[-0.68424344  0.76074266]]\n",
      "   RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[-1.3074701  1.886489 ]]\n",
      "   Output (manual) : [[-0.8649416   0.90466356]]\n",
      "   RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t > 0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c712e",
   "metadata": {},
   "source": [
    "Like the other neural networks that we have seen so far, the weights in a RNN are learnt through backpropagation. The loop introduced in a RNN with many layers may result in one of two opposite problems: exploding gradients or vanishing gradients. The problem is discussed in a [paper](https://arxiv.org/abs/1211.5063) by Pascanu, Mikolov, Bengio. The two outcomes depend on the value of the $W_{hh}$ matrix that are computed multiple times depending on the number of elements in  the sequence we consider to be relevant for the output. If the $|W_{hh}| > 1$ we may face the problem of exploding gradients, on the contrary if $|W_{hh}| < 1$ we may face the problem of vanishing gradients. These problems can be addressed by limiting the length of the sequence we want to take into account for the output. Another approach is to use the Long Short-Term Memory cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e96af",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory network\n",
    "The [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) cell is the equivalent of a layer and solves the problem of the exploding or vanishing gradients by keeping the recurrent edge close to 1. The cell state $C_t$ depends on the previous cell state $C_{t-1}$, the previous output of the hidden units $h_{t-1}$, and on the input in the sequence. The symbol $\\oplus$ in the draw represents the element-wise sumation, and the $\\otimes$ symbol represents the element-wise product. The boxes are called gates and are used to carry out matrix-vector multiplications between the input or the recurrent edge units and the weights to coumpute the preactivations. The result of the preactivation is used by the activation function defined in the cell. The three gates $f_t$, forget gate, $i_t$ input gate, and $o_t$ output gate, use a sigmoid activation function ($\\sigma$).\n",
    "\n",
    "![LSTM](images/long_short-term_memory.svg)\n",
    "(Credit: fdeloche - Own work, Wikipedia, CC BY-SA 4.0)\n",
    "\n",
    "$$f_t = \\sigma(W_{xf}x^{(t)} + W_{hf}h^{(t-1)} + b_f)$$\n",
    "$$i_t = \\sigma(W_{xi}x^{(t)} + W_{hi}h^{(t-1)} + b_i)$$\n",
    "$$\\tilde{C_t} = tanh(W_{xc}x^{(t)} + W_{hc}h^{(t-1)} + b_c)$$\n",
    "$$C^{(t)} = (C^{(t-1)} \\otimes f_t) \\oplus (i_t \\otimes \\tilde{C_t})$$\n",
    "$$o_t = \\sigma(W_{xo}x^{(t)} + W_{ho}h^{(t-1)} + b_o)$$\n",
    "$$h^{(t)} = o_t \\otimes tanh(C^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb3dce",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "We use the PyTorch implementation of the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) to develop a model to determine the sentiment about  a movie by reading a review. This is a many-to-one RNN example, in which an input sequence is used for a classification task. The model is trained from a set of examples from the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) dataset. The IMDB dataset must be downloaded as compressed tar file and openend in a local folder. The reviews are divided into two subfolders, test/ and train/ and these two into /pos for positive reviews and /neg for negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5937a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "954c0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar xvf data/aclImdb_v1.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416f92e",
   "metadata": {},
   "source": [
    "We copy the reviews in the train/ and test/ folder with their sentiments into a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b1b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "movie_df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
    "                movie_df = pd.concat([movie_df, x], ignore_index=False)\n",
    "                \n",
    "movie_df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2b220a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910b264",
   "metadata": {},
   "source": [
    "We might want to save the data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3714c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_df.sample(frac=1, random_state=1).reset_index(drop=True) # returns a randomized dataframe\n",
    "movie_df.to_csv(basepath + '/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7824d79",
   "metadata": {},
   "source": [
    "and then open the file for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5059d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When we started watching this series on cable,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve Biko was a black activist who tried to r...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My short comment for this flick is go pick it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a serious horror fan, I get that certain ma...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Cummings, Laraine Day and Jean Muir sta...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  When we started watching this series on cable,...        1.0\n",
       "1  Steve Biko was a black activist who tried to r...        1.0\n",
       "2  My short comment for this flick is go pick it ...        1.0\n",
       "3  As a serious horror fan, I get that certain ma...        0.0\n",
       "4  Robert Cummings, Laraine Day and Jean Muir sta...        1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath = 'data/aclImdb'\n",
    "movie_df = pd.read_csv(basepath + '/movie_data.csv', encoding='utf-8')\n",
    "movie_df = movie_df.astype({'sentiment': np.float32})\n",
    "movie_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f690aa",
   "metadata": {},
   "source": [
    "We create a custom IMDB dataset from the Python Dataframe. A PyTorch dataset is used to train a model and contains a set of examples, in our case as a set of reviews and sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57703af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        review = item['review']\n",
    "        sentiment = item['sentiment']\n",
    "        if self.transform:\n",
    "            review = self.transform(review)\n",
    "        if self.target_transform:\n",
    "            sentiment = self.target_transform(sentiment)\n",
    "        return review, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffa76",
   "metadata": {},
   "source": [
    "We split the IMDB dataset into a training and test dataset of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e47185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 25000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "movie_dataset = ImdbDataset(df=movie_df)\n",
    "#movie_loader = DataLoader(dataset=review_data, batch_size=4,shuffle=True)\n",
    "\n",
    "train_dataset, test_dataset = random_split(list(movie_dataset), [25000, 25000])\n",
    "print('Train dataset: {0:d}\\nTest dataset: {1:d}'.format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5568480",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We have to prepare the data to train the model. The steps are as follows:\n",
    "\n",
    "1. Split the training dataset into separate training and validation partitions.\n",
    "2. Identify the unique words in the training dataset\n",
    "3. Map each unique word to a unique integer and encode the review text into encoded integers (an index of each unique word)\n",
    "4. Divide the dataset into mini-batches as input to the model\n",
    "\n",
    "We begin by splitting the train dataset into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d181fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 20000\n",
      "Validation dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])\n",
    "print('Train dataset: {0:d}\\nValidation dataset: {1:d}'.format(len(train_dataset), len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2923",
   "metadata": {},
   "source": [
    "We extract the unique words (tokens) from each review in the training set. The reviews may contain html tags that are removed as well as punctuation and other non-letter characters using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f63342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c47530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, sentiment in train_dataset:\n",
    "    tokens = tokenizer(review)\n",
    "    token_counts.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99ab5c",
   "metadata": {},
   "source": [
    "The number of unique words in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5ca03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 71014\n"
     ]
    }
   ],
   "source": [
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f8d9",
   "metadata": {},
   "source": [
    "We create a dictionary using the unique words from the reviews in the training set as keys and an integer as value. The value associated to each word start from 2 since 0 and 1 are reserved for padding and unknown words, that is words not within the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f673bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "count = 2\n",
    "for item in ordered_dict.items():\n",
    "    key = item[0]\n",
    "    value = item[1]\n",
    "    vocab[key] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb365",
   "metadata": {},
   "source": [
    "Words that are contained in the validation or test set but not in the training set are mapped as 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16520f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 473]\n"
     ]
    }
   ],
   "source": [
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59673ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 71016\n"
     ]
    }
   ],
   "source": [
    "print('Vocab-size:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940957da",
   "metadata": {},
   "source": [
    "We map the words in the reviews to integers. The labels do not require any mapping since they are already set to values in {0, 1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48985e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_pipeline = lambda x: [vocab[token] for token in tokenizer(str(x))]\n",
    "\n",
    "def text_pipeline(review):\n",
    "    values = []\n",
    "    for token in tokenizer(str(review)):\n",
    "        if token in vocab:\n",
    "            value = vocab[token]\n",
    "        else:\n",
    "            value = 0\n",
    "        values.append(value)\n",
    "    return values\n",
    "\n",
    "#label_pipeline = lambda x: 1. if x == 'pos' else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7bad5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'an', 'example', 'and', 'this', 'is', 'an', 'equivalence']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('this is an example and this is an equivalence')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce9acbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 7, 35, 473, 3, 11, 7, 35, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc4bbe",
   "metadata": {},
   "source": [
    "We provide an example of how a statement is mapped to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c6069a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11,   7,  35, 473,   3,  11,   7,  35, 473]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text_list = []\n",
    "processed_text = torch.tensor(text_pipeline('this is an example and this is an example'), dtype=torch.int64)\n",
    "my_text_list.append(processed_text)\n",
    "nn.utils.rnn.pad_sequence(my_text_list, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bf984",
   "metadata": {},
   "source": [
    "We implement a collate_fn function to build a batch from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "019c3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _text, _label in batch:\n",
    "        #label_list.append(label_pipeline(_label))\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f501f3f",
   "metadata": {},
   "source": [
    "We print the first batch of 4 sequences of the training set. We can see that the number of tokens, integers different from 0, in each batch is different. Nontheless the length of each batch is the same since it has been padded with 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "349312d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   10,   230,     4,  ...,     0,     0,     0],\n",
      "        [ 2944,     5,     4,  ...,     0,     0,     0],\n",
      "        [   52,    10,   582,  ...,     0,     0,     0],\n",
      "        [   58,    86, 43806,  ...,   142,   746, 43814]])\n",
      "tensor([0., 1., 0., 0.])\n",
      "tensor([132, 413, 242, 986])\n",
      "torch.Size([4, 986])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader)) # takes the first batch\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2bfb6",
   "metadata": {},
   "source": [
    "We create the data loader for the three datasets: train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2a6378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "943ef38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  11,   16,   64,  ...,    0,    0,    0],\n",
       "         [  10,   28,  108,  ...,    0,    0,    0],\n",
       "         [  48,  241,   10,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [2002, 1249, 5966,  ...,    0,    0,    0],\n",
       "         [  10,  138,  108,  ...,    0,    0,    0],\n",
       "         [   4,  121, 3609,  ...,    0,    0,    0]]),\n",
       " tensor([0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.]),\n",
       " tensor([ 109,  372,  191,   62,  746,  123,  178, 1016,  748,  209,  362,  418,\n",
       "          492,   64,  135,  232,  120,  365,  495,  204,  215,  608,  109,  163,\n",
       "          203,  113,  607,  155,  271,  196,  581,  210]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dc607",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "We will represent the words as vectors in a space of real numbers whose dimension n is much smaller than the number of words so that each word can be represented in this space using n real numbers. PyTorch provides some functions to create embeddings. For instance we create an embedding of two tensors of integers. Each tensor represents a batch and each integer represents a word. Each word is mapped to a point in the 3-dimensional embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b438aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.2905,  0.5024,  0.9382],\n",
      "         [ 0.4840,  1.5309,  1.3001],\n",
      "         [-0.7511, -1.2292, -0.7398],\n",
      "         [ 0.4158,  1.6106, -0.3487]],\n",
      "\n",
      "        [[-0.7511, -1.2292, -0.7398],\n",
      "         [ 0.4284,  0.1795, -0.3951],\n",
      "         [ 0.4840,  1.5309,  1.3001],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    " \n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809222b2",
   "metadata": {},
   "source": [
    "## Building an RNN model\n",
    "We create a model with two RNN layers and a final fully connected layer to show how a RNN is built in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f106e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1841],\n",
       "        [ 0.1844],\n",
       "        [ 0.2407],\n",
       "        [-0.1471],\n",
       "        [ 0.2428]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## An example of building a RNN model\n",
    "## with simple RNN layer\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, \n",
    "                          hidden_size, \n",
    "                          num_layers=2, \n",
    "                          batch_first=True)\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = RNN(64, 32) \n",
    "\n",
    "print(model) \n",
    " \n",
    "model(torch.randn(5, 3, 64)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06068b31",
   "metadata": {},
   "source": [
    "## Building an RNN model for the sentiment analysis task\n",
    "We use an LSTM layer to take into account long distance dependencies between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7f50370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71016, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afc05dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d695d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
