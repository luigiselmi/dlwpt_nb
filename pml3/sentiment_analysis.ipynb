{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1623abf9",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "In this notebook we will use a machine learning algorithm to infer the sentiment, positive or negative, about a text. We will use the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) dataset to train our model. The dataset contains 50k movie reviews. We download the dataset and extract the files into a folder. The dataset contains two subfolders train/ and test/ each containing 25k reviews split into two subfolders pos/ and  neg/ with 12500 txt files. Each file contains a short text, the content of the review. The name of the file is created from the review's unique identifier and the score given to the movie. A score equal or higher than 7 is positive, a score equal or lower than 4 is negative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4d656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.23.1\n",
      "Pandas version: 1.4.3\n",
      "PyTorch version: 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"NumPy version: %s\"%np.__version__)\n",
    "print(\"Pandas version: %s\"%pd.__version__)\n",
    "print(\"PyTorch version: %s\"%torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eaa0aa",
   "metadata": {},
   "source": [
    "We copy the reviews with the sentiment in a tabular format so that it will be easier to split and shuffle.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ef2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
    "                df = pd.concat([df, x], ignore_index=False)\n",
    "\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bd33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "df.to_csv(basepath + '/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6200f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9f371",
   "metadata": {},
   "source": [
    "We save the pandas dataframe into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497607f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(basepath + '/movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622ee63",
   "metadata": {},
   "source": [
    "## Bag-of-words representation\n",
    "We want to represent each document by the words that are used. The words come from a dictionary built by analyzing all the documents in the dataset. Each document will be represented by an array that contains the number of times a word from the dictionary has been used. The length of each array is equal to the length of the dictionary. This representation of a set of documents is called [bag-of-words](). In order to create such representation for the reviews we have to tokinize each review and create an array with the number of occurrences for each word. This process is called vectorization. This representation will contain only numbers of occurrences and no words. Scikit-Learn provides the class [CountVectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) to do exactly that. In the bag-of-words model the order of the words in a sentence does not matter, the words are treated as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e9e9e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Election is a Chinese mob movie, or triads in this case',\n",
       "       'I was just watching a Forensic Files marathon on Court TV',\n",
       "       'Police Story is a stunning series of set pieces for Jackie Chan to show his unique talents and bravery'],\n",
       "      dtype='<U102')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_reviews = np.array([df.iloc[i]['review'].split('.')[0] for i in range(0, 3)])\n",
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a0bf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(sample_reviews)\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd2dc3",
   "metadata": {},
   "source": [
    "We can print the vocabulary built from the sample of documents with the index of each word. The index is assigned in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ec6ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'election': 6, 'is': 12, 'chinese': 4, 'mob': 16, 'movie': 17, 'or': 20, 'triads': 31, 'in': 11, 'this': 29, 'case': 2, 'was': 34, 'just': 14, 'watching': 35, 'forensic': 9, 'files': 7, 'marathon': 15, 'on': 19, 'court': 5, 'tv': 32, 'police': 22, 'story': 26, 'stunning': 27, 'series': 23, 'of': 18, 'set': 24, 'pieces': 21, 'for': 8, 'jackie': 13, 'chan': 3, 'to': 30, 'show': 25, 'his': 10, 'unique': 33, 'talents': 28, 'and': 0, 'bravery': 1}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c81c2",
   "metadata": {},
   "source": [
    "The length of the vocabulary is the length of the array that represents each document. Documents with different meaning but with the exact same words will be represented by the same vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cec20642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c706d4",
   "metadata": {},
   "source": [
    "We can print the bag-of-words representation of the sample documents. Each array returned by the vectorizer represents the index of the document, that is a map that links a document to the words that can be found in it in terms of occurrence. Each value in the array represents the frequency of the term in the document, or term frequency *tf(t, d)* where t represents the term and d the document. Most of the time we are interested in the inverted index, a map that links a word to the documents that contain it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1516b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
      " [1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513335ee",
   "metadata": {},
   "source": [
    "## Word relevancy\n",
    "Not all words have the same relevance when we want to classify or rank a text. The least used words are those that provides more information about a document. One way to measure the relevance of a word is the *term frequency-inverse document frequency*, or *tf_idf(t,d)*. The inverse document frequency of a term t, or *idf(t)*, is defined as\n",
    "\n",
    "$$idf(t) = log \\frac{n}{1 + df(t)}$$\n",
    "\n",
    "where n is the total number of documents and *df(t)* is the number of documents that contain the term t (at least once). The term-frequency of a term t, or *tf(t, d)*, is computed as the number of occurrences of a term t in the document d and corresponds to the counts that are returned by the vectorizer. The tf_idf(t,d) of a term t in a document d, that is its relevance, is defined as\n",
    "\n",
    "$$td\\_idf(t, d) = tf(t, d) * idf(t, d)$$\n",
    "\n",
    "Scikit-Learn provides a class [TfidfTransformer](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) that implements the td_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db0c246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.32311233 0.         0.32311233 0.\n",
      "  0.32311233 0.         0.         0.         0.         0.32311233\n",
      "  0.24573525 0.         0.         0.         0.32311233 0.32311233\n",
      "  0.         0.         0.32311233 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32311233\n",
      "  0.         0.32311233 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.33333333 0.         0.33333333 0.         0.\n",
      "  0.         0.         0.33333333 0.33333333 0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.33333333 0.33333333]\n",
      " [0.23851206 0.23851206 0.         0.23851206 0.         0.\n",
      "  0.         0.         0.23851206 0.         0.23851206 0.\n",
      "  0.18139457 0.23851206 0.         0.         0.         0.\n",
      "  0.23851206 0.         0.         0.23851206 0.23851206 0.23851206\n",
      "  0.23851206 0.23851206 0.23851206 0.23851206 0.23851206 0.\n",
      "  0.23851206 0.         0.         0.23851206 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "relevance = tfidf.fit_transform(count.fit_transform(docs)).toarray()\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba23021",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "Before creating the bag of words we have to remove the characters that do not represent words, for example markup from HTML documents, punctuation and special characters as in the last part of the first document. We use regular expressions to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6912d7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef315c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)  # removes HTML markup\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f4ca630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307aacd2",
   "metadata": {},
   "source": [
    "We apply the preprocessing function to all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d0e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812566a",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "After we have removed all the non-word characters from the document we can extract the words by splitting the document using a space as separator. In order to reduce the dictionary we use the Porter stemming algorithm to extract the root of the different forms of words and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "748b2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272d27d",
   "metadata": {},
   "source": [
    "We will also remove stop words, that is very common words that don't help the understanding of the text such as \"the\", \"a\", \"and\", and others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44fe425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d7d51",
   "metadata": {},
   "source": [
    "We can see the result of applying the stemming algorithm and the removal of the (english) stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f50de260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0889b",
   "metadata": {},
   "source": [
    "## Document classification\n",
    "We use a [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) model to classify the reviews into positives or negatives. We have cleaned the reviews by removing markup, punctuations and special characters. Now we divide the dataset into a train set of 25k reviews and a test set of the same size, like in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f7fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae3694",
   "metadata": {},
   "source": [
    "We use a set of hyperparameters to improve the result of the training and the class [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) from Scikit-Learn to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075dfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "\"\"\"\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\"\"\"\n",
    "\n",
    "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [None],\n",
    "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                     'clf__C': [1.0, 10.0]},\n",
    "                    {'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [stop, None],\n",
    "                     'vect__tokenizer': [tokenizer],\n",
    "                     'vect__use_idf':[False],\n",
    "                     'vect__norm':[None],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                  'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa2df276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001DAA19B00D0&gt;,\n",
       "                                              &lt;function tokenizer_porter at 0x000001DA...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001DAA19B00D0&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001DAA19B00D0&gt;,\n",
       "                                              &lt;function tokenizer_porter at 0x000001DA...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x000001DAA19B00D0&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001DAA19B00D0>,\n",
       "                                              <function tokenizer_porter at 0x000001DA...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001DAA19B00D0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9928745",
   "metadata": {},
   "source": [
    "After the search is completed we have the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1699c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001DAA19B00D0>}\n",
      "CV Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936dcfb1",
   "metadata": {},
   "source": [
    "We test the logistic regression model, using the best hyperparameters combination, on the test set. We can see that the model is able to correctly classify a review 90% of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95745d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb44e8",
   "metadata": {},
   "source": [
    "## Online algorithm\n",
    "It takes 10 minutes to process 25k words in a laptop. For larger datasets in case we do not have access to a cluster of computers to parallelize the computation the approach is to fit the model incrementally on small batches of documents. We clean and tokenize the data using a function similar to the preprocessor function we have used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057ba0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1f936",
   "metadata": {},
   "source": [
    "We define an iterator function to return one review at a time from the CSV file that we have previously created using the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3065b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869841a2",
   "metadata": {},
   "source": [
    "We test the iterator on the CSV file that returns the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c71ab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path=basepath + '/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfaab8",
   "metadata": {},
   "source": [
    "We define a function that uses the iterator to return a batch of documemts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40ba5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0479eb1",
   "metadata": {},
   "source": [
    "Since it is assumed we cannot load the dataset in memory we cannot use the CountVectorizer class to map a review to a vector. Scikit-Learn provides the [HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) that maps a set of documents to a matrix of word occurrences without the need to load a dictionary in memory. We set the number of features to store the occurrences of each word to a large number, e.g. $2^{21} = 2097152$, to reduce the collissions, that is different words that are mapped to the same hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baee17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path=basepath + '/movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edbdff",
   "metadata": {},
   "source": [
    "Now we can start the incremental learning process using batches of 1000 documents in 45 loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b4fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4869f53",
   "metadata": {},
   "source": [
    "At the end of the training process we can use the last 5 batches to evaluate the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70f1b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72eaec",
   "metadata": {},
   "source": [
    "The performance of the online process is close to the one we achieved with the data loaded into memory. Now we can use the 5 batches to finalize the traiining of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c359d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33bc3d",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108d95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
